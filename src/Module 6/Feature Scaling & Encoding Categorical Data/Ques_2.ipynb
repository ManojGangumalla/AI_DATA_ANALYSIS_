{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Sex_label_encoded  Sex_female  Sex_male\n",
      "0    male                  1       False      True\n",
      "1  female                  0        True     False\n",
      "2  female                  0        True     False\n",
      "3  female                  0        True     False\n",
      "4    male                  1       False      True\n",
      "   Survived    Pclass       Age     SibSp     Parch      Fare  \\\n",
      "0  -0.82702  0.911232 -0.530377  0.524570 -0.505895 -0.518978   \n",
      "1   1.20916 -1.476364  0.571831  0.524570 -0.505895  0.691897   \n",
      "2   1.20916  0.911232 -0.254825 -0.551703 -0.505895 -0.506214   \n",
      "3   1.20916 -1.476364  0.365167  0.524570 -0.505895  0.348049   \n",
      "4  -0.82702  0.911232  0.365167 -0.551703 -0.505895 -0.503850   \n",
      "\n",
      "   Sex_label_encoded  \n",
      "0           0.759051  \n",
      "1          -1.317434  \n",
      "2          -1.317434  \n",
      "3          -1.317434  \n",
      "4           0.759051  \n",
      "   Sex_male  Embarked_Q  Embarked_S\n",
      "0      True       False        True\n",
      "1     False       False       False\n",
      "2     False       False        True\n",
      "3     False       False        True\n",
      "4      True       False        True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12254/1989357214.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsorted categories are not supported for numerical categories",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Assume: 3rd class < 2nd class < 1st class\u001b[39;00m\n\u001b[1;32m     60\u001b[0m ordinal_encoder \u001b[38;5;241m=\u001b[39m OrdinalEncoder(categories\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m---> 61\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPclass_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mordinal_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPclass_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Question 9: Impact of Scaling on Different Algorithms\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Task: Investigate the impact of different scaling techniques on a decision tree model and compare it with a SVM.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:1515\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_value should only be set when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle_unknown is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_encoded_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1511\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munknown_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m     )\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[0;32m-> 1515\u001b[0m fit_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_and_ignore_missing_for_infrequent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices \u001b[38;5;241m=\u001b[39m fit_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1523\u001b[0m cardinalities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(categories) \u001b[38;5;28;01mfor\u001b[39;00m categories \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:155\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, ensure_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m    153\u001b[0m     stop_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(sorted_cats[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(sorted_cats[:stop_idx] \u001b[38;5;241m!=\u001b[39m cats[:stop_idx]):\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    158\u001b[0m     diff \u001b[38;5;241m=\u001b[39m _check_unknown(Xi, cats)\n",
      "\u001b[0;31mValueError\u001b[0m: Unsorted categories are not supported for numerical categories"
     ]
    }
   ],
   "source": [
    "# Question 5: Label Encoding vs One-Hot Encoding\n",
    "# Task: Show the difference between Label Encoding and One-Hot Encoding on the Titanic dataset for the 'Sex' feature.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['Sex_label_encoded'] = label_encoder.fit_transform(df['Sex'])\n",
    "\n",
    "# One-Hot Encoding\n",
    "onehot_encoded = pd.get_dummies(df['Sex'], prefix='Sex')\n",
    "\n",
    "# Combine for comparison\n",
    "df_encoded = pd.concat([df[['Sex']], df['Sex_label_encoded'], onehot_encoded], axis=1)\n",
    "print(df_encoded.head())\n",
    "\n",
    "\n",
    "# Question 6: Combining Feature Scaling Techniques\n",
    "# Task: Demonstrate combining Min-Max Scaling and Standardization for the same datasetand explain the results.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_features = df.select_dtypes(include=['float64', 'int64']).drop(columns=['PassengerId']).dropna()\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "minmax_scaled = MinMaxScaler().fit_transform(numeric_features)\n",
    "\n",
    "# Apply Standardization on the Min-Max scaled data\n",
    "standardized = StandardScaler().fit_transform(minmax_scaled)\n",
    "\n",
    "# Convert to DataFrame\n",
    "combined_scaled_df = pd.DataFrame(standardized, columns=numeric_features.columns)\n",
    "print(combined_scaled_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 7: Handling Multiple Categorical Features\n",
    "# Task: Handle multiple categorical features ('Sex', 'Embarked') from the Titanic dataset using One-Hot Encoding.\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# One-Hot Encode 'Sex' and 'Embarked'\n",
    "df_onehot = pd.get_dummies(df[['Sex', 'Embarked']], drop_first=True)\n",
    "print(df_onehot.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 8: Ordinal Encoding for Ranked Categories\n",
    "# Task: Ordinal encode 'Pclass' (Passenger class) from the Titanic dataset considering passenger class as a ranked feature.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Assume: 3rd class < 2nd class < 1st class\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[3, 2, 1]])\n",
    "df['Pclass_encoded'] = ordinal_encoder.fit_transform(df[['Pclass']])\n",
    "\n",
    "print(df[['Pclass', 'Pclass_encoded']].head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 9: Impact of Scaling on Different Algorithms\n",
    "# Task: Investigate the impact of different scaling techniques on a decision tree model and compare it with a SVM.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Clean dataset for modeling\n",
    "df_model = df[['Sex', 'Pclass', 'Age', 'Fare', 'Survived']].dropna()\n",
    "df_model['Sex'] = LabelEncoder().fit_transform(df_model['Sex'])\n",
    "\n",
    "X = df_model[['Sex', 'Pclass', 'Age', 'Fare']]\n",
    "y = df_model['Survived']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().transform(X_test)\n",
    "\n",
    "# Train models\n",
    "tree = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "svm = SVC().fit(X_train_std, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Decision Tree Accuracy (Unscaled):\", accuracy_score(y_test, tree.predict(X_test)))\n",
    "print(\"SVM Accuracy (Standardized):\", accuracy_score(y_test, svm.predict(X_test_std)))\n",
    "\n",
    "\n",
    "\n",
    "# Question 10: Custom Transformations for Categorical Features\n",
    "# Task: Implement a custom transformation function for encoding high cardinality categorical features efficiently.\n",
    "def frequency_encode(column):\n",
    "    freq = column.value_counts(normalize=True)\n",
    "    return column.map(freq)\n",
    "\n",
    "# Simulate a high-cardinality feature\n",
    "df['Cabin'] = df['Cabin'].fillna('Unknown')  # Cabin has many unique values\n",
    "df['Cabin_encoded'] = frequency_encode(df['Cabin'])\n",
    "\n",
    "print(df[['Cabin', 'Cabin_encoded']].head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
