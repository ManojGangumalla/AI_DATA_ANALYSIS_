{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Automated Data Profiling\n",
    "\n",
    "**Steps**:\n",
    "1. Using Pandas-Profiling\n",
    "    - Generate a profile report for an existing CSV file.\n",
    "    - Customize the profile report to include correlations.\n",
    "    - Profile a specific subset of columns.\n",
    "2. Using Great Expectations\n",
    "    - Create a basic expectation suite for your data.\n",
    "    - Validate data against an expectation suite.\n",
    "    - Add multiple expectations to a suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnMapExpectation\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecution_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasExecutionEngine\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations.dataset'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.dataset import PandasDataset\n",
    "from great_expectations.expectations.core import ColumnMapExpectation\n",
    "from great_expectations.execution_engine import PandasExecutionEngine\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Define a custom expectation class\n",
    "# -----------------------------\n",
    "class ExpectColumnValuesToBeEven(ColumnMapExpectation):\n",
    "    \"\"\"Expect column values to be even numbers.\"\"\"\n",
    "\n",
    "    # Use PandasExecutionEngine\n",
    "    map_metric = \"column_values.even\"\n",
    "\n",
    "    def validate_configuration(self, configuration):\n",
    "        # Optional: add configuration validation if needed\n",
    "        return super().validate_configuration(configuration)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Register custom metric for \"even\" check\n",
    "# -----------------------------\n",
    "from great_expectations.expectations.metrics import column_map_metric\n",
    "from great_expectations.execution_engine import PandasExecutionEngine\n",
    "\n",
    "@column_map_metric(engine=PandasExecutionEngine)\n",
    "def column_values_even(series, **kwargs):\n",
    "    return series.apply(lambda x: (x % 2 == 0) if pd.notnull(x) else True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Create sample DataFrame and wrap in GE dataset\n",
    "# -----------------------------\n",
    "data = {\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'income': [50000, 60000, 75000, None, 100000]\n",
    "}\n",
    "\n",
    "# Create GE dataset from pandas DataFrame\n",
    "ge_df = ge.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "# Add the custom expectation method to the GE dataset instance\n",
    "ge_df.add_expectation(ExpectColumnValuesToBeEven)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Use the custom expectation on 'age' and 'income' columns\n",
    "# -----------------------------\n",
    "print(\"Validate 'age' column for even numbers:\")\n",
    "result_age = ge_df.expect_column_values_to_be_even(\"age\")\n",
    "print(result_age)\n",
    "\n",
    "print(\"\\nValidate 'income' column for even numbers:\")\n",
    "result_income = ge_df.expect_column_values_to_be_even(\"income\")\n",
    "print(result_income)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-time Monitoring of Data Quality\n",
    "\n",
    "**Steps**:\n",
    "1. Setting up Alerts for Quality Drops\n",
    "    - Use the logging library to set up a basic alert on failed expectations.\n",
    "    - Implementing alerts using email notifications.\n",
    "    - Using a dashboard like Grafana for visual alerts.\n",
    "        - Note: Example assumes integration with a monitoring system\n",
    "        - Alert setup would involve creating a data source and alert rule in Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataContextError",
     "evalue": "Datasource is not a FluentDatasource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataContextError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m datasource_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_pandas_datasource\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datasource_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m context\u001b[38;5;241m.\u001b[39mlist_datasources():\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDatasource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPandasExecutionEngine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_connectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruntime_data_connector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRuntimeDataConnector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_identifiers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault_identifier_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m batch_request \u001b[38;5;241m=\u001b[39m RuntimeBatchRequest(\n\u001b[1;32m     39\u001b[0m     datasource_name\u001b[38;5;241m=\u001b[39mdatasource_name,\n\u001b[1;32m     40\u001b[0m     data_connector_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime_data_connector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     batch_identifiers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_identifier_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_id\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m suite_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrealtime_suite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:718\u001b[0m, in \u001b[0;36mAbstractDataContext.add_datasource\u001b[0;34m(self, name, initialize, datasource, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;129m@new_argument\u001b[39m(\n\u001b[1;32m    692\u001b[0m     argument_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasource\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    693\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.15.49\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FluentDatasource \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add a new Datasource to the data context, with configuration provided as kwargs.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    --Documentation--\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m        Datasource instance added.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitialize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:758\u001b[0m, in \u001b[0;36mAbstractDataContext._add_datasource\u001b[0;34m(self, name, initialize, datasource, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_fluent_datasource(\n\u001b[1;32m    755\u001b[0m         datasource\u001b[38;5;241m=\u001b[39mdatasource,\n\u001b[1;32m    756\u001b[0m     )\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataContextError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasource is not a FluentDatasource\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# noqa: TRY003 # FIXME CoP\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasource\n",
      "\u001b[0;31mDataContextError\u001b[0m: Datasource is not a FluentDatasource"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Setup logging alerts\n",
    "# --------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"data_quality_alerts.log\",\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Load data and setup Great Expectations\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"sample_data.csv\")  # Replace with your CSV file path\n",
    "\n",
    "context = ge.get_context()\n",
    "\n",
    "datasource_name = \"my_pandas_datasource\"\n",
    "if datasource_name not in context.list_datasources():\n",
    "    context.add_datasource(\n",
    "        name=datasource_name,\n",
    "        class_name=\"Datasource\",\n",
    "        execution_engine={\"class_name\": \"PandasExecutionEngine\"},\n",
    "        data_connectors={\n",
    "            \"runtime_data_connector\": {\n",
    "                \"class_name\": \"RuntimeDataConnector\",\n",
    "                \"batch_identifiers\": [\"default_identifier_name\"]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=datasource_name,\n",
    "    data_connector_name=\"runtime_data_connector\",\n",
    "    data_asset_name=\"realtime_data\",\n",
    "    runtime_parameters={\"batch_data\": df},\n",
    "    batch_identifiers={\"default_identifier_name\": \"default_id\"},\n",
    ")\n",
    "\n",
    "suite_name = \"realtime_suite\"\n",
    "if suite_name not in context.list_expectation_suites():\n",
    "    context.create_expectation_suite(suite_name)\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=suite_name\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Define expectations\n",
    "# --------------------------\n",
    "validator.expect_column_values_to_not_be_null(\"age\")\n",
    "validator.expect_column_values_to_be_between(\"age\", min_value=18, max_value=90)\n",
    "validator.expect_column_values_to_not_be_null(\"income\")\n",
    "\n",
    "validator.save_expectation_suite()\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Run validation and log failures\n",
    "# --------------------------\n",
    "results = validator.validate()\n",
    "\n",
    "if not results.success:\n",
    "    logging.warning(\"⚠️ Data validation failed!\")\n",
    "    for result in results[\"results\"]:\n",
    "        if not result[\"success\"]:\n",
    "            expectation = result['expectation_config']['expectation_type']\n",
    "            kwargs = result['expectation_config']['kwargs']\n",
    "            logging.warning(f\"❌ Failed Expectation: {expectation} on {kwargs}\")\n",
    "else:\n",
    "    logging.info(\"✅ Data validation passed.\")\n",
    "\n",
    "# --------------------------\n",
    "# Step 5: Email alert function\n",
    "# --------------------------\n",
    "def send_email_alert(subject, body, to_email):\n",
    "    from_email = \"your_email@gmail.com\"      # Replace with your email\n",
    "    password = \"your_app_password\"            # Use app password if 2FA enabled\n",
    "\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = from_email\n",
    "    msg[\"To\"] = to_email\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465)\n",
    "        server.login(from_email, password)\n",
    "        server.sendmail(from_email, to_email, msg.as_string())\n",
    "        server.quit()\n",
    "        print(\"📧 Email sent!\")\n",
    "    except Exception as e:\n",
    "        print(\"Email failed:\", e)\n",
    "\n",
    "# --------------------------\n",
    "# Step 6: Send email if validation fails\n",
    "# --------------------------\n",
    "if not results.success:\n",
    "    send_email_alert(\n",
    "        subject=\"Data Quality Alert 🚨\",\n",
    "        body=\"One or more data quality expectations failed. Please check logs for details.\",\n",
    "        to_email=\"recipient@example.com\"   # Replace with recipient email\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# Step 7: Notes for Grafana integration\n",
    "# --------------------------\n",
    "\"\"\"\n",
    "To integrate with Grafana for visual alerts:\n",
    "\n",
    "- Use Promtail to ship the 'data_quality_alerts.log' file to Loki (Grafana’s log backend).\n",
    "- Create a Loki datasource in Grafana.\n",
    "- Create alerts in Grafana based on log queries matching \"Data validation failed\" or specific expectation failures.\n",
    "\n",
    "Example Promtail config snippet:\n",
    "\n",
    "server:\n",
    "  http_listen_port: 9080\n",
    "  grpc_listen_port: 0\n",
    "\n",
    "positions:\n",
    "  filename: /tmp/positions.yaml\n",
    "\n",
    "clients:\n",
    "  - url: http://localhost:3100/loki/api/v1/push\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: data_quality_logs\n",
    "    static_configs:\n",
    "      - targets:\n",
    "          - localhost\n",
    "        labels:\n",
    "          job: data_quality\n",
    "          __path__: /path/to/data_quality_alerts.log\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using AI for Data Quality Monitoring\n",
    "**Steps**:\n",
    "1. Basic AI Models for Monitoring\n",
    "    - Train a simple anomaly detection model using Isolation Forest.\n",
    "    - Use a simple custom function based AI logic for outlier detection.\n",
    "    - Creating a monitoring function that utilizes a pre-trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Anomalies detected by Isolation Forest:\n",
      "    age  income  anomaly_iforest\n",
      "5  1000   -5000               -1\n",
      "\n",
      "⚠️ Rule-based outliers:\n",
      "    age  income  anomaly_iforest  anomaly_rule\n",
      "5  1000   -5000               -1          True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- anomaly_iforest\n- anomaly_rule\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ---------------------------------------\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Example: Save data and run monitoring\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ---------------------------------------\u001b[39;00m\n\u001b[1;32m     61\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mmonitor_data_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mmonitor_data_quality\u001b[0;34m(new_data_path, model_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(new_data_path)\n\u001b[1;32m     48\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m---> 49\u001b[0m new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m anomalies \u001b[38;5;241m=\u001b[39m new_df[new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m anomalies\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:420\u001b[0m, in \u001b[0;36mIsolationForest.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03mPredict if a particular sample is an outlier or not.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        model.predict(X)\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 420\u001b[0m decision_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m is_inlier \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(decision_func, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    422\u001b[0m is_inlier[decision_func \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:474\u001b[0m, in \u001b[0;36mIsolationForest.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03mAverage anomaly score of X of the base classifiers.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m        model.decision_function(X)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# We subtract self.offset_ to make 0 be the threshold value for being\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# an outlier:\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py:519\u001b[0m, in \u001b[0;36mIsolationForest.score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03mOpposite of the anomaly score defined in the original paper.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m        model.score(X)\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_samples(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[1;32m   2836\u001b[0m     _estimator,\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m   2844\u001b[0m ):\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[1;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[1;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- anomaly_iforest\n- anomaly_rule\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Train Isolation Forest\n",
    "# -------------------------------\n",
    "data = {\n",
    "    \"age\": [25, 30, 35, 40, 45, 1000],  # 1000 is an outlier\n",
    "    \"income\": [50000, 60000, 75000, 80000, 90000, -5000]  # -5000 is an outlier\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "clf = IsolationForest(contamination=0.2, random_state=42)\n",
    "clf.fit(df)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(clf, \"isolation_forest_model.pkl\")\n",
    "\n",
    "# Predict anomalies on training data\n",
    "df['anomaly_iforest'] = clf.predict(df)  # -1 = anomaly, 1 = normal\n",
    "\n",
    "print(\"🔍 Anomalies detected by Isolation Forest:\")\n",
    "print(df[df['anomaly_iforest'] == -1])\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 2: Simple rule-based outlier function\n",
    "# -----------------------------------------\n",
    "def rule_based_outlier_detection(df):\n",
    "    flags = []\n",
    "    for _, row in df.iterrows():\n",
    "        age_outlier = row['age'] < 18 or row['age'] > 100\n",
    "        income_outlier = row['income'] < 10000 or row['income'] > 200000\n",
    "        flags.append(age_outlier or income_outlier)\n",
    "    return flags\n",
    "\n",
    "df['anomaly_rule'] = rule_based_outlier_detection(df)\n",
    "\n",
    "print(\"\\n⚠️ Rule-based outliers:\")\n",
    "print(df[df['anomaly_rule']])\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 3: Monitoring function with model\n",
    "# ---------------------------------------\n",
    "def monitor_data_quality(new_data_path: str, model_path: str = \"isolation_forest_model.pkl\"):\n",
    "    new_df = pd.read_csv(new_data_path)\n",
    "    model = joblib.load(model_path)\n",
    "    new_df['anomaly'] = model.predict(new_df)\n",
    "    anomalies = new_df[new_df['anomaly'] == -1]\n",
    "    \n",
    "    if not anomalies.empty:\n",
    "        print(f\"\\n🚨 ALERT: {len(anomalies)} anomalies detected in new data!\")\n",
    "        print(anomalies)\n",
    "    else:\n",
    "        print(\"\\n✅ No anomalies detected in new data.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Example: Save data and run monitoring\n",
    "# ---------------------------------------\n",
    "df.to_csv(\"new_data.csv\", index=False)\n",
    "monitor_data_quality(\"new_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
