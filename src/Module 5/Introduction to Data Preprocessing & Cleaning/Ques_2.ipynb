{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values and their count:\n",
      "Feature1    1\n",
      "Feature2    1\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "Feature1    0\n",
      "Feature2    0\n",
      "Feature3    0\n",
      "Target      0\n",
      "dtype: int64\n",
      "\n",
      "Accuracy without handling missing values: 0.0000\n",
      "\n",
      "Accuracy after handling missing values: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Feature1': [5, 7, 8, np.nan, 10, 7, 5, 6],\n",
    "    'Feature2': [1, 2, 1, 3, 4, np.nan, 2, 1],\n",
    "    'Feature3': [3, 2, 1, 4, 3, 2, 4, 5],\n",
    "    'Target': [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Task 1: Identify columns with missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Columns with missing values and their count:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Task 2: Replace missing values with mean (for numerical columns) or mode (for categorical columns)\n",
    "# Identifying numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Replace missing numerical values with mean\n",
    "df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "\n",
    "# Verify if missing values are handled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Task 3: Compare model performance with and without handling missing values\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model without handling missing values (using the original dataset)\n",
    "df_original = pd.DataFrame(data)  # Load original dataset\n",
    "X_original = df_original.drop('Target', axis=1)\n",
    "y_original = df_original['Target']\n",
    "\n",
    "# Impute missing values in the original dataset before splitting into train/test\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_original_imputed = imputer.fit_transform(X_original)\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original_imputed, y_original, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model on original dataset (with imputation)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_orig, y_train_orig)\n",
    "y_pred_orig = model.predict(X_test_orig)\n",
    "\n",
    "# Evaluate accuracy on original dataset\n",
    "accuracy_without_handling = accuracy_score(y_test_orig, y_pred_orig)\n",
    "print(f\"\\nAccuracy without handling missing values: {accuracy_without_handling:.4f}\")\n",
    "\n",
    "# Train model after handling missing values (already done in the df)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy after handling missing values\n",
    "accuracy_with_handling = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy after handling missing values: {accuracy_with_handling:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Duplicate Data: Repeated data points can skew analysis and model results.\n",
    "#     Task 1: Identify and remove duplicate entries from a dataset using a programming language or tool.\n",
    "#     Task 2: Document the before-and-after dataset shape to understand the impact of duplicates.\n",
    "#     Task 3: Explain to a classmate how duplicate data can affect prediction accuracy.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Incorrect Data Types: Data stored in incorrect formats can lead to parsing errors or incorrect analysis.\n",
    "#     Task 1: Convert a column of string numbers to integers in a dataset.\n",
    "#     Task 2: Identify and correct columns with inconsistent data types in a dataset.\n",
    "#     Task 3: Discuss why correct data types are critical for feature engineering.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Outliers & Inconsistencies: Irregularities in data can mislead statistical analysis and model predictions.\n",
    "#     Task 1: Visualize a dataset and identify outliers using a boxplot.\n",
    "#     Task 2: Remove or adjust outliers and re-analyze the dataset.\n",
    "#     Task 3: Research and report on a technique for handling outliers effectively.\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
