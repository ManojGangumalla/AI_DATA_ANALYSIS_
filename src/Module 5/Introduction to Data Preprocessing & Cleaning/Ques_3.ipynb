{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Dataset:\n",
      "   ID     Name   Age         City   Salary\n",
      "0   1    Alice  25.0          NaN      NaN\n",
      "1   2      Bob  30.0          NaN      NaN\n",
      "2   3  Charlie  35.0     New York  70000.0\n",
      "3   4    David  40.0  Los Angeles  80000.0\n",
      "4   5      NaN   NaN      Chicago  90000.0\n",
      "5   6      NaN   NaN      Houston  95000.0\n",
      "\n",
      "Challenges Faced During Data Collection:\n",
      "1. Handling missing values: Missing data can occur during data collection. We handled this by using 'outer' join to keep all records.\n",
      "2. Duplicate records: We checked for duplicates using the 'duplicated' method and removed them if necessary.\n",
      "3. Inconsistent data formats: We ensured consistency in column data types during the merge.\n",
      "\n",
      "Missing Values in Dataset:\n",
      "ID        0\n",
      "Name      2\n",
      "Age       2\n",
      "City      2\n",
      "Salary    2\n",
      "dtype: int64\n",
      "\n",
      "Number of Duplicate Records:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Task 1: Collect data from two different sources and merge them\n",
    "\n",
    "# Simulate two data sources as DataFrames\n",
    "data1 = {\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40]\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    'ID': [3, 4, 5, 6],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n",
    "    'Salary': [70000, 80000, 90000, 95000]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge the datasets on the 'ID' column (common key)\n",
    "merged_df = pd.merge(df1, df2, on='ID', how='outer')  # 'outer' join to keep all records\n",
    "\n",
    "# Display the merged dataset\n",
    "print(\"Merged Dataset:\")\n",
    "print(merged_df)\n",
    "\n",
    "# Task 2: Validate the integrity of the collected datasets\n",
    "\n",
    "# Check for missing values in the merged dataset\n",
    "missing_values = merged_df.isnull().sum()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_records = merged_df.duplicated().sum()\n",
    "\n",
    "# Task 3: Challenges and how they were addressed\n",
    "print(\"\\nChallenges Faced During Data Collection:\")\n",
    "print(\"1. Handling missing values: Missing data can occur during data collection. We handled this by using 'outer' join to keep all records.\")\n",
    "print(\"2. Duplicate records: We checked for duplicates using the 'duplicated' method and removed them if necessary.\")\n",
    "print(\"3. Inconsistent data formats: We ensured consistency in column data types during the merge.\")\n",
    "\n",
    "# Output missing values and duplicate count\n",
    "print(\"\\nMissing Values in Dataset:\")\n",
    "print(missing_values)\n",
    "print(\"\\nNumber of Duplicate Records:\")\n",
    "print(duplicate_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning: Addressing missing values, duplicates, incorrect types, and outliers.\n",
    "# Task 1: Clean a given dataset and document the changes made.\n",
    "# Task 2: Create a checklist to ensure comprehensive data cleaning in future projects.\n",
    "# Task 3: Collaborate with a peer to clean a new dataset and present your solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Transformation: Modifying data to fit specific analytical requirements.\n",
    "# Task 1: Transform a date column into separate 'day', 'month', and 'year' columns.\n",
    "# Task 2: Apply normalization to a dataset feature and confirm the changes.\n",
    "# Task 3: Discuss the importance of data transformation in model interpretability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Scaling: Adjusting data features to a common scale.\n",
    "# Task 1: Apply Min-Max scaling to a dataset.\n",
    "# Task 2: Standardize a dataset and visualize the changes with a histogram.\n",
    "# Task 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering: Creating new features from existing ones to improve model accuracy.\n",
    "# Task 1: Create a new synthetic feature from existing dataset features.\n",
    "# Task 2: Evaluate the impact of new features on model accuracy.\n",
    "# Task 3: Read an academic paper on feature engineering techniques and present the findings.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
