{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture to Monitor Data Quality Over Time\n",
    "\n",
    "**Description**: Design a monitoring system in Python that checks and logs data quality metrics (accuracy, completeness) for a dataset over time.\n",
    "\n",
    "**Steps to follow:**\n",
    "1. Implement a Scheduled Script:\n",
    "    - Use schedule library to periodically run a script.\n",
    "2. Script to Calculate Metrics:\n",
    "    - For simplicity, use a function calculate_quality_metrics() that calculates and logs metrics such as missing rate or mismatch rate.\n",
    "3. Store Logs:\n",
    "    - Use Python's logging library to save these metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x756721103670>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import unittest\n",
    "from io import StringIO\n",
    "\n",
    "# Setup logging (configurable log level)\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logging.basicConfig(\n",
    "    filename=\"data_quality.log\",\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def calculate_quality_metrics(file_path):\n",
    "    \"\"\"\n",
    "    Load dataset and calculate:\n",
    "    - Overall missing data rate (%)\n",
    "    - Mismatch rate between 'value_1' and 'value_2' if columns exist\n",
    "    Logs the results or errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        logging.error(f\"Parsing error: File format not supported or corrupted: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "    missing_rate = df.isnull().mean().mean() * 100\n",
    "\n",
    "    mismatch_rate = None\n",
    "    if \"value_1\" in df.columns and \"value_2\" in df.columns:\n",
    "        mismatch_rate = (df[\"value_1\"] != df[\"value_2\"]).mean() * 100\n",
    "\n",
    "    log_msg = f\"Missing Rate: {missing_rate:.2f}%\"\n",
    "    if mismatch_rate is not None:\n",
    "        log_msg += f\", Mismatch Rate: {mismatch_rate:.2f}%\"\n",
    "\n",
    "    logging.info(log_msg)\n",
    "    return missing_rate, mismatch_rate\n",
    "\n",
    "# Unit test class\n",
    "class TestQualityMetrics(unittest.TestCase):\n",
    "\n",
    "    def test_missing_rate(self):\n",
    "        csv_data = StringIO(\"value_1,value_2\\n1,1\\n,2\\n3,\")\n",
    "        df = pd.read_csv(csv_data)\n",
    "        missing_rate = df.isnull().mean().mean() * 100\n",
    "        self.assertAlmostEqual(missing_rate, 33.33, places=1)\n",
    "\n",
    "    def test_mismatch_rate(self):\n",
    "        csv_data = StringIO(\"value_1,value_2\\n1,1\\n2,3\\n3,3\\n4,4\")\n",
    "        df = pd.read_csv(csv_data)\n",
    "        mismatch_rate = (df[\"value_1\"] != df[\"value_2\"]).mean() * 100\n",
    "        self.assertAlmostEqual(mismatch_rate, 25.0)\n",
    "\n",
    "    def test_file_not_found(self):\n",
    "        result = calculate_quality_metrics(\"non_existent_file.csv\")\n",
    "        self.assertIsNone(result)\n",
    "\n",
    "    def test_valid_file(self):\n",
    "        # Create a small CSV file for testing\n",
    "        test_csv = \"test_data.csv\"\n",
    "        df_test = pd.DataFrame({\n",
    "            \"value_1\": [1, 2, None],\n",
    "            \"value_2\": [1, 3, None]\n",
    "        })\n",
    "        df_test.to_csv(test_csv, index=False)\n",
    "        result = calculate_quality_metrics(test_csv)\n",
    "        self.assertIsNotNone(result)\n",
    "        import os\n",
    "        os.remove(test_csv)\n",
    "\n",
    "# Run unit tests in Jupyter/IPython\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
