{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data loaded successfully.\n",
      "INFO:__main__:Mean imputation successful.\n",
      "INFO:__main__:KNN imputation successful.\n",
      "INFO:__main__:Standardization successful.\n",
      "INFO:__main__:Min-Max scaling successful.\n",
      "INFO:__main__:Robust scaling successful.\n",
      "INFO:__main__:Highly correlated features removed: ['Longitude']\n",
      "INFO:__main__:Selected top mutual information features: ['AveOccup', 'AveRooms', 'Latitude', 'Longitude', 'MedInc']\n",
      "INFO:__main__:Low variance features removed.\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        data = fetch_california_housing(as_frame=True)\n",
    "        df = data.frame\n",
    "        logger.info(\"Data loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to load dataset: %s\", e)\n",
    "        raise\n",
    "\n",
    "def impute_data(df):\n",
    "    try:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "        logger.info(\"Mean imputation successful.\")\n",
    "        return df_imputed\n",
    "    except ValueError as e:\n",
    "        logger.error(\"ValueError during imputation: %s\", e)\n",
    "        raise\n",
    "def knn_impute_data(df):\n",
    "    try:\n",
    "        imputer = KNNImputer(n_neighbors=3)\n",
    "        df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "        logger.info(\"KNN imputation successful.\")\n",
    "        return df_imputed\n",
    "    except ValueError as e:\n",
    "        logger.error(\"ValueError during KNN imputation: %s\", e)\n",
    "        raise\n",
    "\n",
    "def scale_data(df):\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        logger.info(\"Standardization successful.\")\n",
    "        return scaled_df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in standardization: %s\", e)\n",
    "        raise\n",
    "\n",
    "def minmax_scale_data(df):\n",
    "    try:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        logger.info(\"Min-Max scaling successful.\")\n",
    "        return scaled_df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in Min-Max scaling: %s\", e)\n",
    "        raise\n",
    "def robust_scale_data(df):\n",
    "    try:\n",
    "        scaler = RobustScaler()\n",
    "        scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        logger.info(\"Robust scaling successful.\")\n",
    "        return scaled_df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in robust scaling: %s\", e)\n",
    "        raise\n",
    "\n",
    "def remove_highly_correlated(df, threshold=0.9):\n",
    "    try:\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "        df_reduced = df.drop(columns=to_drop)\n",
    "        logger.info(\"Highly correlated features removed: %s\", to_drop)\n",
    "        return df_reduced\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in correlation removal: %s\", e)\n",
    "        raise\n",
    "\n",
    "def select_features_by_mi(X, y, top_k=5):\n",
    "    try:\n",
    "        mi = mutual_info_classif(X, y)\n",
    "        selected_features = X.columns[np.argsort(mi)[-top_k:]]\n",
    "        logger.info(\"Selected top mutual information features: %s\", list(selected_features))\n",
    "        return X[selected_features]\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in mutual information feature selection: %s\", e)\n",
    "        raise\n",
    "\n",
    "def apply_variance_threshold(df, threshold=0.01):\n",
    "    try:\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        reduced_df = pd.DataFrame(selector.fit_transform(df), columns=df.columns[selector.get_support()])\n",
    "        logger.info(\"Low variance features removed.\")\n",
    "        return reduced_df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in variance thresholding: %s\", e)\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    df = load_data()\n",
    "    df_missing = df.copy()\n",
    "    df_missing.iloc[0:10, 0] = np.nan  # introduce missing values\n",
    "\n",
    "    df_imputed = impute_data(df_missing)\n",
    "    df_knn_imputed = knn_impute_data(df_missing)\n",
    "\n",
    "    df_scaled = scale_data(df_imputed)\n",
    "    df_minmax_scaled = minmax_scale_data(df_imputed)\n",
    "    df_robust_scaled = robust_scale_data(df_imputed)\n",
    "\n",
    "    df_uncorrelated = remove_highly_correlated(df_scaled)\n",
    "\n",
    "    X = df_scaled.drop(columns=['MedHouseVal'])\n",
    "    y = df_scaled['MedHouseVal'] > df_scaled['MedHouseVal'].median()\n",
    "    X_mi = select_features_by_mi(X, y)\n",
    "    X_vt = apply_variance_threshold(X)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
